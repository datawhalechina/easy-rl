# DQN

## åŸç†ç®€ä»‹

DQNæ˜¯Q-leanningç®—æ³•çš„ä¼˜åŒ–å’Œå»¶ä¼¸ï¼ŒQ-leaningä¸­ä½¿ç”¨æœ‰é™çš„Qè¡¨å­˜å‚¨å€¼çš„ä¿¡æ¯ï¼Œè€ŒDQNä¸­åˆ™ç”¨ç¥ç»ç½‘ç»œæ›¿ä»£Qè¡¨å­˜å‚¨ä¿¡æ¯ï¼Œè¿™æ ·æ›´é€‚ç”¨äºé«˜ç»´çš„æƒ…å†µï¼Œç›¸å…³çŸ¥è¯†åŸºç¡€å¯å‚è€ƒ[datawhaleæå®æ¯…ç¬”è®°-Qå­¦ä¹ ](https://datawhalechina.github.io/easy-rl/#/chapter6/chapter6)ã€‚

è®ºæ–‡æ–¹é¢ä¸»è¦å¯ä»¥å‚è€ƒä¸¤ç¯‡ï¼Œä¸€ç¯‡å°±æ˜¯2013å¹´è°·æ­ŒDeepMindå›¢é˜Ÿçš„[Playing Atari with Deep Reinforcement Learning](https://www.cs.toronto.edu/~vmnih/docs/dqn.pdf)ï¼Œä¸€ç¯‡æ˜¯ä¹Ÿæ˜¯ä»–ä»¬å›¢é˜Ÿåæ¥åœ¨Natureæ‚å¿—ä¸Šå‘è¡¨çš„[Human-level control through deep reinforcement learning](https://web.stanford.edu/class/psych209/Readings/MnihEtAlHassibis15NatureControlDeepRL.pdf)ã€‚åè€…åœ¨ç®—æ³•å±‚é¢å¢åŠ target q-netï¼Œä¹Ÿå¯ä»¥å«åšNature DQNã€‚

Nature DQNä½¿ç”¨äº†ä¸¤ä¸ªQç½‘ç»œï¼Œä¸€ä¸ªå½“å‰Qç½‘ç»œğ‘„ç”¨æ¥é€‰æ‹©åŠ¨ä½œï¼Œæ›´æ–°æ¨¡å‹å‚æ•°ï¼Œå¦ä¸€ä¸ªç›®æ ‡Qç½‘ç»œğ‘„â€²ç”¨äºè®¡ç®—ç›®æ ‡Qå€¼ã€‚ç›®æ ‡Qç½‘ç»œçš„ç½‘ç»œå‚æ•°ä¸éœ€è¦è¿­ä»£æ›´æ–°ï¼Œè€Œæ˜¯æ¯éš”ä¸€æ®µæ—¶é—´ä»å½“å‰Qç½‘ç»œğ‘„å¤åˆ¶è¿‡æ¥ï¼Œå³å»¶æ—¶æ›´æ–°ï¼Œè¿™æ ·å¯ä»¥å‡å°‘ç›®æ ‡Qå€¼å’Œå½“å‰çš„Qå€¼ç›¸å…³æ€§ã€‚

è¦æ³¨æ„çš„æ˜¯ï¼Œä¸¤ä¸ªQç½‘ç»œçš„ç»“æ„æ˜¯ä¸€æ¨¡ä¸€æ ·çš„ã€‚è¿™æ ·æ‰å¯ä»¥å¤åˆ¶ç½‘ç»œå‚æ•°ã€‚Nature DQNå’Œ[Playing Atari with Deep Reinforcement Learning](https://www.cs.toronto.edu/~vmnih/docs/dqn.pdf)ç›¸æ¯”ï¼Œé™¤äº†ç”¨ä¸€ä¸ªæ–°çš„ç›¸åŒç»“æ„çš„ç›®æ ‡Qç½‘ç»œæ¥è®¡ç®—ç›®æ ‡Qå€¼ä»¥å¤–ï¼Œå…¶ä½™éƒ¨åˆ†åŸºæœ¬æ˜¯å®Œå…¨ç›¸åŒçš„ã€‚ç»†èŠ‚ä¹Ÿå¯å‚è€ƒ[å¼ºåŒ–å­¦ä¹ ï¼ˆä¹ï¼‰Deep Q-Learningè¿›é˜¶ä¹‹Nature DQN](https://www.cnblogs.com/pinard/p/9756075.html)ã€‚

https://blog.csdn.net/JohnJim0/article/details/109557173)

## ä¼ªä»£ç 

<img src="assets/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0pvaG5KaW0w,size_16,color_FFFFFF,t_70.png" alt="img" style="zoom:50%;" />

## ä»£ç å®ç°

### RLæ¥å£

é¦–å…ˆæ˜¯å¼ºåŒ–å­¦ä¹ è®­ç»ƒçš„åŸºæœ¬æ¥å£ï¼Œå³é€šç”¨çš„è®­ç»ƒæ¨¡å¼ï¼š
```python
for i_episode in range(MAX_EPISODES):
	state = env.reset() # resetç¯å¢ƒçŠ¶æ€
	for i_step in range(MAX_STEPS):
    action = agent.choose_action(state) # æ ¹æ®å½“å‰ç¯å¢ƒstateé€‰æ‹©action
    next_state, reward, done, _ = env.step(action) # æ›´æ–°ç¯å¢ƒå‚æ•°
    agent.memory.push(state, action, reward, next_state, done) # å°†stateç­‰è¿™äº›transitionå­˜å…¥memory
    agent.update() # æ¯æ­¥æ›´æ–°ç½‘ç»œ
    state = next_state # è·³è½¬åˆ°ä¸‹ä¸€ä¸ªçŠ¶æ€
    if done:
    	break        
```
æ¯ä¸ªepisodeåŠ ä¸€ä¸ªMAX_STEPSï¼Œä¹Ÿå¯ä»¥ä½¿ç”¨while not done, åŠ è¿™ä¸ªmax_stepsæœ‰æ—¶æ˜¯å› ä¸ºæ¯”å¦‚gymç¯å¢ƒè®­ç»ƒç›®æ ‡å°±æ˜¯åœ¨200ä¸ªstepä¸‹è¾¾åˆ°200çš„rewardï¼Œæˆ–è€…æ˜¯å½“å®Œæˆä¸€ä¸ªepisodeçš„æ­¥æ•°è¾ƒå¤šæ—¶ä¹Ÿå¯ä»¥è®¾ç½®ï¼ŒåŸºæœ¬æµç¨‹è·Ÿæ‰€æœ‰ä¼ªä»£ç ä¸€è‡´ï¼Œå¦‚ä¸‹ï¼š
1. agenté€‰æ‹©åŠ¨ä½œ
2. ç¯å¢ƒæ ¹æ®agentçš„åŠ¨ä½œåé¦ˆå‡ºnext_stateå’Œreward
3. agentè¿›è¡Œæ›´æ–°ï¼Œå¦‚æœ‰memoryå°±ä¼šå°†transition(åŒ…å«stateï¼Œrewardï¼Œactionç­‰)å­˜å…¥memoryä¸­
4. è·³è½¬åˆ°ä¸‹ä¸€ä¸ªçŠ¶æ€
5. å¦‚æœdoneäº†ï¼Œå°±è·³å‡ºå¾ªç¯ï¼Œè¿›è¡Œä¸‹ä¸€ä¸ªepisodeçš„è®­ç»ƒã€‚

æƒ³è¦å®ç°å®Œæ•´çš„ç®—æ³•è¿˜éœ€è¦åˆ›å»ºQnetï¼ŒReplaybufferç­‰ç±»

### ä¸¤ä¸ªQç½‘ç»œ

ä¸Šæ–‡è®²äº†Nature DQNä¸­æœ‰ä¸¤ä¸ªQç½‘ç»œï¼Œä¸€ä¸ªæ˜¯policy_netï¼Œä¸€ä¸ªæ˜¯å»¶æ—¶æ›´æ–°çš„target_netï¼Œä¸¤ä¸ªç½‘ç»œçš„ç»“æ„æ˜¯ä¸€æ¨¡ä¸€æ ·çš„ï¼Œå¦‚ä¸‹(è§```model.py```)ï¼Œæ³¨æ„DQNä½¿ç”¨çš„Qnetå°±æ˜¯å…¨è¿æ¥ç½‘ç»œå³FCHï¼š
```python
import torch.nn as nn
import torch.nn.functional as F

class FCN(nn.Module):
    def __init__(self, n_states=4, n_actions=18):
        """ åˆå§‹åŒ–qç½‘ç»œï¼Œä¸ºå…¨è¿æ¥ç½‘ç»œ
            n_states: è¾“å…¥çš„featureå³ç¯å¢ƒçš„stateæ•°ç›®
            n_actions: è¾“å‡ºçš„actionæ€»ä¸ªæ•°
        """
        super(FCN, self).__init__()
        self.fc1 = nn.Linear(n_states, 128) # è¾“å…¥å±‚
        self.fc2 = nn.Linear(128, 128) # éšè—å±‚
        self.fc3 = nn.Linear(128, n_actions) # è¾“å‡ºå±‚
        
    def forward(self, x):
        # å„å±‚å¯¹åº”çš„æ¿€æ´»å‡½æ•°
        x = F.relu(self.fc1(x)) 
        x = F.relu(self.fc2(x))
        return self.fc3(x)
```
è¾“å…¥ä¸ºn_statesï¼Œè¾“å‡ºä¸ºn_actionsï¼ŒåŒ…å«ä¸€ä¸ª128ç»´åº¦çš„éšè—å±‚ï¼Œè¿™é‡Œæ ¹æ®éœ€è¦å¯å¢åŠ éšè—å±‚ç»´åº¦å’Œæ•°é‡ï¼Œç„¶åä¸€èˆ¬ä½¿ç”¨reluæ¿€æ´»å‡½æ•°ï¼Œè¿™é‡Œè·Ÿæ·±åº¦å­¦ä¹ çš„ç½‘è·¯è®¾ç½®æ˜¯ä¸€æ ·çš„ã€‚

### Replay Buffer

ç„¶åå°±æ˜¯Replay Memoryäº†ï¼Œå…¶ä½œç”¨ä¸»è¦æ˜¯æ˜¯å…‹æœç»éªŒæ•°æ®çš„ç›¸å…³æ€§ï¼ˆcorrelated dataï¼‰å’Œéå¹³ç¨³åˆ†å¸ƒï¼ˆnon-stationary distributionï¼‰é—®é¢˜ï¼Œå®ç°å¦‚ä¸‹(è§```memory.py```)ï¼š

```python
import random
import numpy as np

class ReplayBuffer:
    
    def __init__(self, capacity):
        self.capacity = capacity
        self.buffer = []
        self.position = 0
    
    def push(self, state, action, reward, next_state, done):
        if len(self.buffer) < self.capacity:
            self.buffer.append(None)
        self.buffer[self.position] = (state, action, reward, next_state, done)
        self.position = (self.position + 1) % self.capacity
    
    def sample(self, batch_size):
        batch = random.sample(self.buffer, batch_size)
        state, action, reward, next_state, done =  zip(*batch)
        return state, action, reward, next_state, done
    
    def __len__(self):
        return len(self.buffer)
```

å‚æ•°capacityè¡¨ç¤ºbufferçš„å®¹é‡ï¼Œä¸»è¦åŒ…æ‹¬pushå’Œsampleä¸¤ä¸ªæ­¥éª¤ï¼Œpushæ˜¯å°†transitionsæ”¾åˆ°memoryä¸­ï¼Œsampleæ˜¯ä»memoryéšæœºæŠ½å–ä¸€äº›transitionã€‚

### Agentç±»

åœ¨```agent.py```ä¸­æˆ‘ä»¬å®šä¹‰å¼ºåŒ–å­¦ä¹ ç®—æ³•ç±»ï¼ŒåŒ…æ‹¬```choose_action```(é€‰æ‹©åŠ¨ä½œï¼Œä½¿ç”¨e-greedyç­–ç•¥æ—¶ä¼šå¤šä¸€ä¸ª```predict```å‡½æ•°ï¼Œä¸‹é¢ä¼šå°†åˆ°)å’Œ```update```(æ›´æ–°)ç­‰å‡½æ•°ã€‚

åœ¨ç±»ä¸­å»ºç«‹ä¸¤ä¸ªç½‘ç»œï¼Œä»¥åŠoptimizerå’Œmemoryï¼Œ

```python
self.policy_net = MLP(n_states, n_actions,hidden_dim=cfg.hidden_dim).to(self.device)
self.target_net = MLP(n_states, n_actions,hidden_dim=cfg.hidden_dim).to(self.device)
for target_param, param in zip(self.target_net.parameters(),self.policy_net.parameters()): # copy params from policy net
    target_param.data.copy_(param.data)
self.optimizer = optim.Adam(self.policy_net.parameters(), lr=cfg.lr)
self.memory = ReplayBuffer(cfg.memory_capacity)
```
ç„¶åæ˜¯é€‰æ‹©actionï¼š

```python
def choose_action(self, state):
        '''é€‰æ‹©åŠ¨ä½œ
        '''
    self.frame_idx += 1
    if random.random() > self.epsilon(self.frame_idx):
        action = self.predict(state)
    else:
        action = random.randrange(self.n_actions)
    return action
```

è¿™é‡Œä½¿ç”¨e-greedyç­–ç•¥ï¼Œå³è®¾ç½®ä¸€ä¸ªå‚æ•°epsilonï¼Œå¦‚æœç”Ÿæˆçš„éšæœºæ•°å¤§äºepsilonï¼Œå°±æ ¹æ®ç½‘ç»œé¢„æµ‹çš„é€‰æ‹©actionï¼Œå¦åˆ™è¿˜æ˜¯éšæœºé€‰æ‹©actionï¼Œè¿™ä¸ªepsilonæ˜¯ä¼šé€æ¸å‡å°çš„ï¼Œå¯ä»¥ä½¿ç”¨çº¿æ€§æˆ–è€…æŒ‡æ•°å‡å°çš„æ–¹å¼ï¼Œä½†ä¸ä¼šå‡å°åˆ°é›¶ï¼Œè¿™æ ·åœ¨è®­ç»ƒç¨³å®šæ—¶è¿˜èƒ½ä¿æŒä¸€å®šçš„æ¢ç´¢ï¼Œè¿™éƒ¨åˆ†å¯ä»¥å­¦ä¹ æ¢ç´¢ä¸åˆ©ç”¨(exploration and exploition)ç›¸å…³çŸ¥è¯†ã€‚

ä¸Šé¢è®²åˆ°çš„é¢„æµ‹å‡½æ•°å…¶å®å°±æ˜¯æ ¹æ®stateé€‰å–qå€¼æœ€å¤§çš„actionï¼Œå¦‚ä¸‹ï¼š

```python
def predict(self,state):
    with torch.no_grad():
        state = torch.tensor([state], device=self.device, dtype=torch.float32)
        q_values = self.policy_net(state)
        action = q_values.max(1)[1].item()
```

ç„¶åæ˜¯æ›´æ–°å‡½æ•°äº†ï¼š

```python
def update(self):

        if len(self.memory) < self.batch_size:
            return
        # ä»memoryä¸­éšæœºé‡‡æ ·transition
        state_batch, action_batch, reward_batch, next_state_batch, done_batch = self.memory.sample(
            self.batch_size)
        '''è½¬ä¸ºå¼ é‡
        ä¾‹å¦‚tensor([[-4.5543e-02, -2.3910e-01,  1.8344e-02,  2.3158e-01],...,[-1.8615e-02, -2.3921e-01, -1.1791e-02,  2.3400e-01]])'''
        state_batch = torch.tensor(
            state_batch, device=self.device, dtype=torch.float)
        action_batch = torch.tensor(action_batch, device=self.device).unsqueeze(
            1)  # ä¾‹å¦‚tensor([[1],...,[0]])
        reward_batch = torch.tensor(
            reward_batch, device=self.device, dtype=torch.float)  # tensor([1., 1.,...,1])
        next_state_batch = torch.tensor(
            next_state_batch, device=self.device, dtype=torch.float)
        done_batch = torch.tensor(np.float32(
            done_batch), device=self.device)

        '''è®¡ç®—å½“å‰(s_t,a)å¯¹åº”çš„Q(s_t, a)'''
        '''torch.gather:å¯¹äºa=torch.Tensor([[1,2],[3,4]]),é‚£ä¹ˆa.gather(1,torch.Tensor([[0],[1]]))=torch.Tensor([[1],[3]])'''
        q_values = self.policy_net(state_batch).gather(
            dim=1, index=action_batch)  # ç­‰ä»·äºself.forward
        # è®¡ç®—æ‰€æœ‰next statesçš„V(s_{t+1})ï¼Œå³é€šè¿‡target_netä¸­é€‰å–rewardæœ€å¤§çš„å¯¹åº”states
        next_q_values = self.target_net(next_state_batch).max(
            1)[0].detach()  # æ¯”å¦‚tensor([ 0.0060, -0.0171,...,])
        # è®¡ç®— expected_q_value
        # å¯¹äºç»ˆæ­¢çŠ¶æ€ï¼Œæ­¤æ—¶done_batch[0]=1, å¯¹åº”çš„expected_q_valueç­‰äºreward
        expected_q_values = reward_batch + \
            self.gamma * next_q_values * (1-done_batch)
        # self.loss = F.smooth_l1_loss(q_values,expected_q_values.unsqueeze(1)) # è®¡ç®— Huber loss
        loss = nn.MSELoss()(q_values, expected_q_values.unsqueeze(1))  # è®¡ç®— å‡æ–¹è¯¯å·®loss
        # ä¼˜åŒ–æ¨¡å‹
        self.optimizer.zero_grad()  # zero_gradæ¸…é™¤ä¸Šä¸€æ­¥æ‰€æœ‰æ—§çš„gradients from the last step
        # loss.backward()ä½¿ç”¨backpropagationè®¡ç®—lossç›¸å¯¹äºæ‰€æœ‰parameters(éœ€è¦gradients)çš„å¾®åˆ†
        loss.backward()
        # for param in self.policy_net.parameters():  # clipé˜²æ­¢æ¢¯åº¦çˆ†ç‚¸
        #     param.grad.data.clamp_(-1, 1)
        self.optimizer.step()  # æ›´æ–°æ¨¡å‹
```

æ›´æ–°éµå¾ªä¼ªä»£ç çš„ä»¥ä¸‹éƒ¨åˆ†ï¼š

<img src="assets/image-20210507162813393.png" alt="image-20210507162813393" style="zoom:50%;" />

é¦–å…ˆä»replay bufferä¸­é€‰å–ä¸€ä¸ªbatchçš„æ•°æ®ï¼Œè®¡ç®—lossï¼Œç„¶åè¿›è¡Œminibatch SGDã€‚

ç„¶åæ˜¯ä¿å­˜ä¸åŠ è½½æ¨¡å‹çš„éƒ¨åˆ†ï¼Œå¦‚ä¸‹ï¼š

```python
def save(self, path):
        torch.save(self.target_net.state_dict(), path+'dqn_checkpoint.pth')
def load(self, path):
    self.target_net.load_state_dict(torch.load(path+'dqn_checkpoint.pth'))
    for target_param, param in zip(self.target_net.parameters(), self.policy_net.parameters()):
        param.data.copy_(target_param.data)
```



### å®éªŒç»“æœ

è®­ç»ƒç»“æœå¦‚ä¸‹ï¼š

<img src="assets/train_rewards_curve.png" alt="train_rewards_curve" style="zoom: 67%;" />

<img src="assets/eval_rewards_curve.png" alt="eval_rewards_curve" style="zoom:67%;" />

## å‚è€ƒ

[with torch.no_grad()](https://www.jianshu.com/p/1cea017f5d11)

