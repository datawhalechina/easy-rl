# DQN

## åŸç†ç®€ä»‹
DQNæ˜¯Q-leanningç®—æ³•çš„ä¼˜åŒ–å’Œå»¶ä¼¸ï¼ŒQ-leaningä¸­ä½¿ç”¨æœ‰é™çš„Qè¡¨å­˜å‚¨å€¼çš„ä¿¡æ¯ï¼Œè€ŒDQNä¸­åˆ™ç”¨ç¥ç»ç½‘ç»œæ›¿ä»£Qè¡¨å­˜å‚¨ä¿¡æ¯ï¼Œè¿™æ ·æ›´é€‚ç”¨äºé«˜ç»´çš„æƒ…å†µï¼Œç›¸å…³çŸ¥è¯†åŸºç¡€å¯å‚è€ƒ[datawhaleæå®æ¯…ç¬”è®°-Qå­¦ä¹ ](https://datawhalechina.github.io/easy-rl/#/chapter6/chapter6)ã€‚

è®ºæ–‡æ–¹é¢ä¸»è¦å¯ä»¥å‚è€ƒä¸¤ç¯‡ï¼Œä¸€ç¯‡å°±æ˜¯2013å¹´è°·æ­ŒDeepMindå›¢é˜Ÿçš„[Playing Atari with Deep Reinforcement Learning](https://www.cs.toronto.edu/~vmnih/docs/dqn.pdf)ï¼Œä¸€ç¯‡æ˜¯ä¹Ÿæ˜¯ä»–ä»¬å›¢é˜Ÿåæ¥åœ¨Natureæ‚å¿—ä¸Šå‘è¡¨çš„[Human-level control through deep reinforcement learning](https://web.stanford.edu/class/psych209/Readings/MnihEtAlHassibis15NatureControlDeepRL.pdf)ã€‚åè€…åœ¨ç®—æ³•å±‚é¢å¢åŠ target q-netï¼Œä¹Ÿå¯ä»¥å«åšNature DQNã€‚

Nature DQNä½¿ç”¨äº†ä¸¤ä¸ªQç½‘ç»œï¼Œä¸€ä¸ªå½“å‰Qç½‘ç»œğ‘„ç”¨æ¥é€‰æ‹©åŠ¨ä½œï¼Œæ›´æ–°æ¨¡å‹å‚æ•°ï¼Œå¦ä¸€ä¸ªç›®æ ‡Qç½‘ç»œğ‘„â€²ç”¨äºè®¡ç®—ç›®æ ‡Qå€¼ã€‚ç›®æ ‡Qç½‘ç»œçš„ç½‘ç»œå‚æ•°ä¸éœ€è¦è¿­ä»£æ›´æ–°ï¼Œè€Œæ˜¯æ¯éš”ä¸€æ®µæ—¶é—´ä»å½“å‰Qç½‘ç»œğ‘„å¤åˆ¶è¿‡æ¥ï¼Œå³å»¶æ—¶æ›´æ–°ï¼Œè¿™æ ·å¯ä»¥å‡å°‘ç›®æ ‡Qå€¼å’Œå½“å‰çš„Qå€¼ç›¸å…³æ€§ã€‚

è¦æ³¨æ„çš„æ˜¯ï¼Œä¸¤ä¸ªQç½‘ç»œçš„ç»“æ„æ˜¯ä¸€æ¨¡ä¸€æ ·çš„ã€‚è¿™æ ·æ‰å¯ä»¥å¤åˆ¶ç½‘ç»œå‚æ•°ã€‚Nature DQNå’Œ[Playing Atari with Deep Reinforcement Learning](https://www.cs.toronto.edu/~vmnih/docs/dqn.pdf)ç›¸æ¯”ï¼Œé™¤äº†ç”¨ä¸€ä¸ªæ–°çš„ç›¸åŒç»“æ„çš„ç›®æ ‡Qç½‘ç»œæ¥è®¡ç®—ç›®æ ‡Qå€¼ä»¥å¤–ï¼Œå…¶ä½™éƒ¨åˆ†åŸºæœ¬æ˜¯å®Œå…¨ç›¸åŒçš„ã€‚ç»†èŠ‚ä¹Ÿå¯å‚è€ƒ[å¼ºåŒ–å­¦ä¹ ï¼ˆä¹ï¼‰Deep Q-Learningè¿›é˜¶ä¹‹Nature DQN](https://www.cnblogs.com/pinard/p/9756075.html)ã€‚

https://blog.csdn.net/JohnJim0/article/details/109557173)

## ä¼ªä»£ç 

<img src="assets/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0pvaG5KaW0w,size_16,color_FFFFFF,t_70.png" alt="img" style="zoom:50%;" />

## ä»£ç å®æˆ˜

### RLæ¥å£

é¦–å…ˆæ˜¯å¼ºåŒ–å­¦ä¹ è®­ç»ƒçš„åŸºæœ¬æ¥å£ï¼Œå³é€šç”¨çš„è®­ç»ƒæ¨¡å¼ï¼š
```python
for i_episode in range(MAX_EPISODES):
	state = env.reset() # resetç¯å¢ƒçŠ¶æ€
	for i_step in range(MAX_STEPS):
		 action = agent.choose_action(state) # æ ¹æ®å½“å‰ç¯å¢ƒstateé€‰æ‹©action
         next_state, reward, done, _ = env.step(action) # æ›´æ–°ç¯å¢ƒå‚æ•°
         agent.memory.push(state, action, reward, next_state, done) # å°†stateç­‰è¿™äº›transitionå­˜å…¥memory
         agent.update() # æ¯æ­¥æ›´æ–°ç½‘ç»œ
         state = next_state # è·³è½¬åˆ°ä¸‹ä¸€ä¸ªçŠ¶æ€
         if done:
         	break        
```
å¦‚ä¸Šï¼Œé¦–å…ˆéœ€è¦å¾ªç¯å¤šä¸ªepisodeè®­ç»ƒï¼Œåœ¨æ¯ä¸ªepisodeä¸­ï¼Œé¦–å…ˆéœ€è¦é‡ç½®ç¯å¢ƒï¼Œç„¶åå¼€å§‹æ¢ç´¢ï¼Œæ¯ä¸ªepisodeåŠ ä¸€ä¸ªMAX_STEPS(ä¹Ÿå¯ä»¥ä½¿ç”¨while not done, åŠ è¿™ä¸ªmax_stepsæœ‰æ—¶æ˜¯å› ä¸ºæ¯”å¦‚gymç¯å¢ƒè®­ç»ƒç›®æ ‡å°±æ˜¯åœ¨200ä¸ªstepä¸‹è¾¾åˆ°200çš„reward)ï¼Œæ¥ä¸‹æ¥çš„æµç¨‹å¦‚ä¸‹ï¼š
1. agenté€‰æ‹©åŠ¨ä½œ
2. ç¯å¢ƒæ ¹æ®agentçš„åŠ¨ä½œåé¦ˆå‡ºæ–°çš„stateå’Œreward
3. agentè¿›è¡Œæ›´æ–°ï¼Œå¦‚æœ‰memoryå°±ä¼šå°†transition(åŒ…å«stateï¼Œrewardï¼Œactionç­‰)å­˜å…¥memoryä¸­
4. è·³è½¬åˆ°ä¸‹ä¸€ä¸ªçŠ¶æ€
å¦‚æœæå‰doneäº†ï¼Œå°±è·³å‡ºforå¾ªç¯ï¼Œè¿›è¡Œä¸‹ä¸€ä¸ªepisodeçš„è®­ç»ƒã€‚

### ä¸¤ä¸ªQç½‘ç»œ
å‰é¢è®²äº†Nature DQNä¸­æœ‰ä¸¤ä¸ªQç½‘ç»œï¼Œä¸€ä¸ªæ˜¯policy_netï¼Œä¸€ä¸ªæ˜¯å»¶æ—¶æ›´æ–°çš„target_netï¼Œä¸¤ä¸ªç½‘ç»œçš„ç»“æ„æ˜¯ä¸€æ¨¡ä¸€æ ·çš„ï¼Œå¦‚ä¸‹(è§```model.py```)ï¼š
```python
import torch.nn as nn
import torch.nn.functional as F

class FCN(nn.Module):
    def __init__(self, state_dim=4, action_dim=18):
        """ åˆå§‹åŒ–qç½‘ç»œï¼Œä¸ºå…¨è¿æ¥ç½‘ç»œ
            state_dim: è¾“å…¥çš„featureå³ç¯å¢ƒçš„stateæ•°ç›®
            action_dim: è¾“å‡ºçš„actionæ€»ä¸ªæ•°
        """
        super(FCN, self).__init__()
        self.fc1 = nn.Linear(state_dim, 128) # è¾“å…¥å±‚
        self.fc2 = nn.Linear(128, 128) # éšè—å±‚
        self.fc3 = nn.Linear(128, action_dim) # è¾“å‡ºå±‚
        
    def forward(self, x):
        # å„å±‚å¯¹åº”çš„æ¿€æ´»å‡½æ•°
        x = F.relu(self.fc1(x)) 
        x = F.relu(self.fc2(x))
        return self.fc3(x)
```
è¾“å…¥ä¸ºstateï¼Œè¾“å‡ºä¸ºactionï¼Œæ³¨æ„æ ¹æ®stateå’Œactionçš„ç»´åº¦è°ƒæ•´éšè—å±‚çš„å±‚æ•°ï¼Œè¿™é‡Œè®¾ä¸º128

åœ¨```agent.py```ä¸­æˆ‘ä»¬å®šä¹‰å¼ºåŒ–å­¦ä¹ ç®—æ³•ï¼ŒåŒ…æ‹¬```choose_action```å’Œ```update```ä¸¤ä¸ªä¸»è¦å‡½æ•°ï¼Œåˆå§‹åŒ–ä¸­ï¼š
```python
self.policy_net = FCN(state_dim, action_dim).to(self.device)
self.target_net = FCN(state_dim, action_dim).to(self.device)
# target_netçš„åˆå§‹æ¨¡å‹å‚æ•°å®Œå…¨å¤åˆ¶policy_net
self.target_net.load_state_dict(self.policy_net.state_dict())
self.target_net.eval()  # ä¸å¯ç”¨ BatchNormalization å’Œ Dropout
# å¯æŸ¥parameters()ä¸state_dict()çš„åŒºåˆ«ï¼Œå‰è€…require_grad=True
```
å¯ä»¥çœ‹åˆ°policy_netè·Ÿtarget_netç»“æ„å’Œåˆå§‹å‚æ•°ä¸€æ ·ï¼Œä½†åœ¨æ›´æ–°çš„æ—¶å€™targetæ˜¯æ¯éš”ä¸€æ®µepisodeæ›´æ–°çš„ï¼Œå¦‚ä¸‹(è§```main.py```)ï¼š
```python
# æ›´æ–°target networkï¼Œå¤åˆ¶DQNä¸­çš„æ‰€æœ‰weights and biases
if i_episode % cfg.target_update == 0:
	agent.target_net.load_state_dict(agent.policy_net.state_dict())
```
å¯ä»¥è°ƒæ•´```cfg.target_update```ï¼Œæ³¨æ„è¯¥å˜é‡ä¸è¦è°ƒå¾—å¤ªå¤§ï¼Œå¦åˆ™ä¼šæ”¶æ•›å¾ˆæ…¢ï¼Œæˆ‘ä»¬æœ€åä¿å­˜çš„æ¨¡å‹ä¹Ÿæ˜¯è¿™ä¸ªtarget_netï¼Œå¦‚ä¸‹(è§```agent.py```)ï¼š
```python
def save_model(self,path):
	torch.save(self.target_net.state_dict(), path)
```
### Replay Memory
ç„¶åå°±æ˜¯Replay Memoryäº†ï¼Œå¦‚ä¸‹(è§```memory.py```)ï¼š
```python
import random
import numpy as np

class ReplayBuffer:
    
    def __init__(self, capacity):
        self.capacity = capacity
        self.buffer = []
        self.position = 0
    
    def push(self, state, action, reward, next_state, done):
        if len(self.buffer) < self.capacity:
            self.buffer.append(None)
        self.buffer[self.position] = (state, action, reward, next_state, done)
        self.position = (self.position + 1) % self.capacity
    
    def sample(self, batch_size):
        batch = random.sample(self.buffer, batch_size)
        state, action, reward, next_state, done =  zip(*batch)
        return state, action, reward, next_state, done
    
    def __len__(self):
        return len(self.buffer)
```
å…¶å®æ¯”è¾ƒç®€å•ï¼Œä¸»è¦åŒ…æ‹¬pushå’Œsampleä¸¤ä¸ªæ­¥éª¤ï¼Œpushæ˜¯å°†transitionsæ”¾åˆ°memoryä¸­ï¼Œsampleæ˜¯ä»memoryéšæœºæŠ½å–ä¸€äº›transitionã€‚

æœ€åç»“æœå¦‚ä¸‹ï¼š

![rewards_curve_train](assets/rewards_curve_train.png)

## å‚è€ƒ

[with torch.no_grad()](https://www.jianshu.com/p/1cea017f5d11)

