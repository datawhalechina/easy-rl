# 第五章 近端策略优化 (PPO) 算法

## 关键词

- **同策略（on-policy）**：要学习的智能体和与环境交互的智能体是同一个时对应的策略。

- **异策略（off-policy）**：要学习的智能体和与环境交互的智能体不是同一个时对应的策略。

- **重要性采样（important sampling）**：使用另外一种分布，来逼近所求分布的一种方法，在强化学习中通常和蒙特卡洛方法结合使用，公式如下：
$$
    \int f(x) p(x) \mathrm{d} x=\int f(x) \frac{p(x)}{q(x)} q(x) \mathrm{d} x=E_{x \sim q}[f(x){\frac{p(x)}{q(x)}}]=E_{x \sim p}[f(x)]
$$
我们在已知 $q$ 的分布后，可以使用上式计算出从 $p$ 这个分布采样 $x$ 代入 $f$ 以后得到的期望值。

- **近端策略优化（proximal policy optimization，PPO）**：避免在使用重要性采样时由于在 $\theta$ 下的 $p_{\theta}\left(a_{t} | s_{t}\right)$ 与在  $\theta '$ 下的 $p_{\theta'}\left(a_{t} | s_{t}\right)$ 相差太多，导致重要性采样结果偏差较大而采取的算法。具体来说就是在训练的过程中增加一个限制，这个限制对应 $\theta$ 和 $\theta'$ 输出的动作的KL散度，来衡量 $\theta$ 与 $\theta'$ 的相似程度。


## 习题

**5-1** 基于同策略的策略梯度有什么可改进之处？或者说其效率较低的原因在于什么？

经典策略梯度的大部分时间花在数据采样上，即当我们的智能体与环境交互后，我们就要进行策略模型的更新。但是对于一个回合我们仅能更新策略模型一次，更新完后我们就要花时间重新采样数据，然后才能再次进行如上的更新。

所以我们可以使用异策略的方法，即使用另一个不同的策略和演员，与环境进行交互并用所采样的数据进行原先策略的更新。这样等价于使用同一组数据，在同一个回合，我们对整个策略模型更新了多次，这样会更加有效率。

**5-2** 使用重要性采样时需要注意的问题有哪些？

我们可以在重要性采样中将 $p$ 替换为任意的 $q$，但是本质上要求两者的分布不能差太多，即使我们补偿了不同数据分布的权重 $\frac{p(x)}{q(x)}$ 。 $E_{x \sim p}[f(x)]=E_{x \sim q}\left[f(x) \frac{p(x)}{q(x)}\right]$ ，当我们对于两者的采样次数都比较多时，最终的结果会是较为接近的。但是通常我们不会取理想数量的采样数据，所以如果两者的分布相差较大，最后结果的方差将会很大。

**5-3** 基于异策略的重要性采样中的数据是从 $\theta'$ 中采样出来的，从 $\theta$ 换成 $\theta'$ 有什么优势？

使用基于异策略的重要性采样后，我们不用 $\theta$ 与环境交互，而是由另外一个策略 $\theta'$ 进行示范。 $\theta'$ 的任务就是示范给 $\theta$ 看，它和环境交互，告诉 $\theta$ 它与环境交互会发生什么事，以此来训练 $\theta$ 。我们要训练的是 $\theta$ ，$\theta'$ 只负责做示范，负责与环境交互，所以采样出来的数据与 $\theta$ 本身是没有关系的。所以就可以让 $\theta'$ 与环境交互采样大量数据，$\theta$ 可以更新参数多次。一直到 $\theta$ 训练到一定的程度、参数更新多次以后，$\theta'$ 再重新采样，这就是同策略换成异策略的妙处。

**5-4** 在本节中近端策略优化中的KL散度指的是什么？

本质来说，KL散度是一个函数，其度量的是两个动作（对应的参数分别为 $\theta$ 和 $\theta'$ ）间的行为距离，而不是参数距离。这里的行为距离可以理解为在相同状态下输出动作的差距（概率分布上的差距），概率分布即KL散度。


## 面试题

**5-1** 友善的面试官：请问什么是重要性采样呀？

使用另外一种分布，来逼近所求分布的一种方法，算是一种期望修正的方法，公式如下：

$$
\int f(x) p(x) \mathrm{d} x=\int f(x) \frac{p(x)}{q(x)} q(x) \mathrm{d} x=E_{x \sim q}[f(x){\frac{p(x)}{q(x)}}]=E_{x \sim p}[f(x)]
$$

我们在已知 $q$ 的分布后，可以使用上式计算出从 $p$ 分布的期望值。也就可以使用 $q$ 来对 $p$ 进行采样了，即重要性采样。

**5-2** 友善的面试官：请问同策略和异策略的区别是什么？

我可以用一句话概括两者的区别，即生成样本的策略（价值函数）和网络参数更新时的策略（价值函数）是否相同。具体来说，同策略,生成样本的策略（价值函数）与网络更新参数时使用的策略（价值函数）相同。Sarsa算法就是同策略的，其基于当前的策略直接执行一次动作，然后用价值函数的值更新当前的策略，因此生成样本的策略和学习时的策略相同，算法为同策略算法。该算法会遭遇探索-利用窘境，仅利用目前已知的最优选择，可能学不到最优解，不能收敛到局部最优，而加入探索又降低了学习效率。 $\varepsilon$-贪心算法是这种矛盾下的折中，其优点是直接了当、速度快，缺点是不一定能够找到最优策略。异策略，生成样本的策略（价值函数）与网络更新参数时使用的策略（价值函数）不同。例如，Q学习算法在计算下一状态的预期奖励时使用了最大化操作，直接选择最优动作，而当前策略并不一定能选择到最优动作，因此这里生成样本的策略和学习时的策略不同，即异策略算法。

**5-3** 友善的面试官：请简述一下近端策略优化算法。其与信任区域策略优化算法有何关系呢？

近端策略优化算法借鉴了信任区域策略优化算法，通过采用一阶优化，在采样效率、算法表现以及实现和调试的复杂度之间取得了新的平衡。这是因为近端策略优化算法会在每一次迭代中尝试计算新的策略，让损失函数最小化，并且保证每一次新计算出的策略能够和原策略相差不大。换句话说，其为在避免使用重要性采样时由于在 $\theta$ 下的 $p_{\theta}\left(a_{t} | s_{t}\right)$ 与在 $\theta'$ 下的 $p_{\theta'}\left(a_{t} | s_{t}\right)$ 差太多，导致重要性采样结果偏差较大而采取的算法。
